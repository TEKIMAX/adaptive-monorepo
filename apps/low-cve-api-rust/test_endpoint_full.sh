#!/bin/bash
set -e

# Cleanup
docker rm -f rust-api-test || true

# Secrets
export OLLAMA_API_KEY="c5f758681ad0454d97f9d15c021ba73b.Kvwle4MUNOJf8TOXTXyPXJF0"
# Using the API_KEY from .env.local as GEMINI_API_KEY based on file inspection
export GEMINI_API_KEY="AIzaSyBGlq6jIVgDPiIXkmDZJbZF-rvl8J9GZN8" 
export OLLAMA_BASE_URL="https://ollama.com"

echo "Starting container..."
docker run -d --name rust-api-test \
  -e OLLAMA_API_KEY=$OLLAMA_API_KEY \
  -e GEMINI_API_KEY=$GEMINI_API_KEY \
  -e OLLAMA_BASE_URL=$OLLAMA_BASE_URL \
  -p 8080:8080 \
  low-cve-api-rust

echo "Waiting for service..."
sleep 3

echo "========================================"
echo "TEST 1: OLLAMA CLOUD (via /v1/models)"
echo "========================================"
MODELS_JSON=$(curl -s http://localhost:8080/v1/models)
echo "Models: $MODELS_JSON"

# Extract first model
MODEL_ID=$(echo $MODELS_JSON | grep -o '"id":"[^"]*"' | head -n 1 | cut -d'"' -f4)

if [ -z "$MODEL_ID" ]; then
  echo "WARNING: No Ollama models found. Skipping Ollama chat test."
else
  echo "Testing Chat with Model: $MODEL_ID"
  curl -v -N -X POST http://localhost:8080/v1/responses \
    -H "Content-Type: application/json" \
    -d '{
      "model": "'"$MODEL_ID"'",
      "input": [{"type": "message", "role": "user", "content": "Hello Ollama"}]
    }'
fi

echo ""
echo "========================================"
echo "TEST 2: GOOGLE GEMINI (Direct)"
echo "========================================"
# We use a known model name "gemini-1.5-flash" (or "gemini-pro"). The routing uses "gemini" prefix.
# Note: Google's OpenAI endpoint expects specific model names.
echo "Testing Chat with Model: gemini-3-flash-preview"
curl -v -N -X POST http://localhost:8080/v1/responses \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-3-flash-preview",
    "input": [{"type": "message", "role": "user", "content": "Hello Gemini"}]
  }'

echo ""
echo "Done."
docker stop rust-api-test
docker rm rust-api-test
