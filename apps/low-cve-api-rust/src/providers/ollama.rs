use crate::traits::Provider;
use crate::types::*;
use async_trait::async_trait;
use reqwest::Client;
use serde_json::{json, Value};
use std::env;
use std::error::Error;
use std::time::{SystemTime, UNIX_EPOCH};
use tokio::sync::mpsc::Sender;
use uuid::Uuid;
use futures::StreamExt;

pub struct OllamaProvider {
    pub client: Client,
}

impl OllamaProvider {
    pub fn new() -> Self {
        Self { client: Client::new() }
    }
}

#[async_trait]
impl Provider for OllamaProvider {
    fn id(&self) -> &str {
        "ollama"
    }

    fn model_prefix(&self) -> &str {
        "ollama" // matches 'ollama/llama3', etc.
    }

    async fn execute(
        &self,
        req: &CreateResponseRequest,
        headers: &axum::http::HeaderMap,
        sender: Sender<Result<OpenResponseEvent, Box<dyn Error + Send + Sync>>>,
    ) -> Result<(), Box<dyn Error + Send + Sync>> {
        // Allow overriding base URL via header
        let mut endpoint = env::var("OLLAMA_BASE_URL").unwrap_or_else(|_| "http://localhost:11434".to_string());
        if let Some(url) = headers.get("x-ollama-base-url") {
             if let Ok(url_str) = url.to_str() {
                 endpoint = url_str.to_string();
             }
        }
        
        // Normalize model name: remove prefix if needed, or pass as is.
        // req.model might be "ollama/llama3". Ollama expects "llama3".
        let model_name = req.model.replace("ollama/", "");

        let mut messages = Vec::new();
        for item in &req.input {
            match item {
                InputItem::Message { role, content } => {
                     let text_content = match content {
                        InputContent::String(s) => s.clone(),
                        InputContent::Parts(parts) => {
                            parts.iter().map(|p| match p {
                                InputContentPart::Text { text } => text.clone(),
                                InputContentPart::InputText { text } => text.clone(),
                            }).collect::<Vec<_>>().join("")
                        }
                     };
                     messages.push(json!({ "role": role, "content": text_content }));
                }
            }
        }

        let body = json!({
            "model": model_name,
            "messages": messages,
            "stream": true,
            "options": {
                "temperature": req.temperature,
                // Maps max_output_tokens to num_predict if present
                "num_predict": req.max_output_tokens
            }
        });

        let mut request_builder = self.client.post(format!("{}/api/chat", endpoint));

        let mut api_key = env::var("OLLAMA_API_KEY").ok();
        if let Some(key_header) = headers.get("x-ollama-api-key") {
             if let Ok(k) = key_header.to_str() {
                 api_key = Some(k.to_string());
             }
        }

        if let Some(key) = api_key {
            request_builder = request_builder.header("Authorization", format!("Bearer {}", key));
        }

        let res = request_builder
            .json(&body)
            .send()
            .await?;
        
        if !res.status().is_success() {
            let error_text = res.text().await?;
            return Err(format!("Ollama API Error: {}", error_text).into());
        }

        let mut stream = res.bytes_stream();

        let response_id = format!("resp_{}", Uuid::new_v4());
        let item_id = format!("msg_{}", Uuid::new_v4());
        let created_at = SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs();

        let response_res = ResponseResource {
            id: response_id.clone(),
            object: "response".to_string(),
            created_at,
            status: "in_progress".to_string(),
            model: req.model.clone(),
            output: vec![],
            completed_at: None,
            metadata: Some(std::collections::HashMap::from([
                ("ai_generated".to_string(), "true".to_string()),
                ("provider".to_string(), "ollama".to_string())
            ])),
        };

        sender.send(Ok(OpenResponseEvent::ResponseInProgress { response: response_res.clone() })).await?;

        let item = OpenResponseItem::Message {
            id: item_id.clone(),
            role: "assistant".to_string(),
            status: "in_progress".to_string(),
            content: vec![],
        };
        sender.send(Ok(OpenResponseEvent::OutputItemAdded {
            output_index: 0,
            item: item.clone(),
            response_id: response_id.clone(),
        })).await?;

        let mut buffer = String::new();

        while let Some(chunk_res) = stream.next().await {
            let chunk = chunk_res?;
            // Ollama sends JSON objects one per line (NDJSON style), but sometimes in single chunks
            // We need to parse strictly.
            let s = String::from_utf8_lossy(&chunk);
            
            // Ollama stream format: { "model": "...", "created_at": "...", "message": { "role": "assistant", "content": "..." }, "done": false }
            // If we receive multiple objects in one chunk, handle them.
            // Using serde_json::Deserializer might be cleaner, but simple line split usually works request/response
            
            // Note: simple splitting by newline might break if content contains newlines inside JSON string.
            // But Ollama usually sends one JSON object per http chunk or newline separated.
            // For robustness in this MVP, we assume line-delimited or clean chunks.
            
            // A safer approach with bytes is to implement a codec, but let's try line buffer if needed.
            // For now, assuming standard line separation.
            
            // Actually, reqwest bytes_stream chunks might not align with lines.
            // Use a buffer? 
            // For simplicity in this step, let's assume we can parse serde_json from the slice if it's a complete object, 
            // or split by newline. 
            
            // Re-simulating logic from simple provider:
            for line in s.split('\n') {
                if line.trim().is_empty() { continue; }
                
                if let Ok(json) = serde_json::from_str::<Value>(line) {
                    if let Some(content) = json["message"]["content"].as_str() {
                        if !content.is_empty() {
                            sender.send(Ok(OpenResponseEvent::OutputTextDelta {
                                item_id: item_id.clone(),
                                output_index: 0,
                                content_index: 0,
                                delta: content.to_string(),
                                response_id: response_id.clone(),
                            })).await?;
                            buffer.push_str(content);
                        }
                    }
                    if json["done"].as_bool().unwrap_or(false) {
                        // done
                    }
                }
            }
        }

        let item_done = OpenResponseItem::Message {
            id: item_id.clone(),
            role: "assistant".to_string(),
            status: "completed".to_string(),
            content: vec![OutputContentPart::OutputText { text: buffer }],
        };
        sender.send(Ok(OpenResponseEvent::OutputItemDone {
            output_index: 0,
            item: item_done.clone(),
            response_id: response_id.clone(),
        })).await?;

        let mut completed_res = response_res.clone();
        completed_res.status = "completed".to_string();
        completed_res.completed_at = Some(SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs());
        completed_res.output = vec![item_done];

        sender.send(Ok(OpenResponseEvent::ResponseCompleted { response: completed_res })).await?;

        Ok(())
    }

    async fn list_models(&self) -> Result<Vec<crate::types::Model>, Box<dyn Error + Send + Sync>> {
        let endpoint = env::var("OLLAMA_BASE_URL").unwrap_or_else(|_| "http://localhost:11434".to_string());
        
        // Ollama API /api/tags returns { "models": [ ... ] }
        let mut request_builder = self.client.get(format!("{}/api/tags", endpoint));

        if let Ok(api_key) = env::var("OLLAMA_API_KEY") {
            request_builder = request_builder.header("Authorization", format!("Bearer {}", api_key));
        }

        let res = request_builder.send().await?;
        
        if !res.status().is_success() {
            let error_text = res.text().await?;
            return Err(format!("Ollama List Models Error: {}", error_text).into());
        }

        let json: Value = res.json().await?;
        
        let mut models = Vec::new();
        
        if let Some(list) = json["models"].as_array() {
            for item in list {
                if let Some(name) = item["name"].as_str() {
                    // Prepend "ollama/" so the router knows to route it back to this provider
                    // But maybe we should return it *as is* or normalized?
                    // The router expects "ollama/" prefix. If we return "llama3" here, 
                    // the user will send "llama3" and it won't route to ollama.
                    // So we must prefix it.
                    let id = format!("ollama/{}", name);
                    
                    models.push(crate::types::Model {
                        id,
                        object: "model".to_string(),
                        created: SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs(), // Timestamp approximation or parse item["modified_at"]
                        owned_by: "ollama".to_string(),
                    });
                }
            }
        }

        Ok(models)
    }
}
