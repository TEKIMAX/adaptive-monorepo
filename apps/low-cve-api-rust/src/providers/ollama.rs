use async_trait::async_trait;
use crate::traits::Provider;
use crate::types::{CreateResponseRequest, InputContentPart, OpenResponseEvent, OpenResponseItem, ResponseResource, OutputContentPart, InputItem, InputContent, ErrorDetails};
use reqwest::Client;
use std::error::Error;
use tokio::sync::mpsc::Sender;
use axum::http::HeaderMap;
use serde_json::json;
use futures::StreamExt;
use chrono::Utc;
use uuid::Uuid;

#[derive(Clone)]
pub struct OllamaProvider {
    pub client: Client,
    pub base_url: Option<String>,
}

impl OllamaProvider {
    pub fn new(base_url: Option<String>) -> Self {
        Self {
            client: Client::new(),
            base_url,
        }
    }
}

#[async_trait]
impl Provider for OllamaProvider {
    fn model_prefix(&self) -> &str {
        "ollama"
    }

    async fn list_models(&self) -> Result<Vec<crate::types::Model>, Box<dyn Error + Send + Sync>> {
        let base_url = std::env::var("OLLAMA_BASE_URL").unwrap_or_else(|_| "https://ollama.com".to_string());
        let url = format!("{}/api/tags", base_url);

        let res = self.client.get(&url).send().await?;
        
        if !res.status().is_success() {
             return Err(format!("Failed to list models: {}", res.status()).into());
        }

        let body: serde_json::Value = res.json().await?;
        tracing::info!("Ollama list body from {}: {:?}", url, body);
        let models_json = body["models"].as_array();
        
        let mut models = Vec::new();
        if let Some(arr) = models_json {
             for m in arr {
                 let name = m["name"].as_str().unwrap_or_default().to_string();
                 models.push(crate::types::Model {
                     id: format!("ollama/{}", name),
                     object: "model".to_string(),
                     created: 0,
                     owned_by: "ollama".to_string(),
                 });
             }
        }
        
        Ok(models)
    }

    async fn execute(
        &self,
        request: &CreateResponseRequest,
        _headers: &HeaderMap,
        sender: Sender<OpenResponseEvent>,
    ) -> Result<(), Box<dyn Error + Send + Sync>> {
        let model_name = request.model.trim_start_matches("ollama/");
        let base_url = self.base_url.clone().unwrap_or_else(|| std::env::var("OLLAMA_BASE_URL").unwrap_or_else(|_| "https://ollama.com".to_string()));
        let api_key = std::env::var("OLLAMA_API_KEY").unwrap_or_default();
        
        let mut messages = Vec::new();
        for item in &request.input {
            if let InputItem::Message { role, content } = item {
                let content_str = match content {
                    InputContent::String(s) => s.clone(),
                    InputContent::Parts(parts) => {
                         let mut acc = String::new();
                         for part in parts {
                             match part {
                                 InputContentPart::Text { text } => acc.push_str(text),
                                 InputContentPart::InputText { text } => acc.push_str(text),
                                 InputContentPart::InlineData { .. } => {},
                             }
                         }
                         acc
                    }
                };
                messages.push(json!({ "role": role, "content": content_str }));
            }
        }

        let mut ollama_req = json!({
            "model": model_name,
            "messages": messages,
            "stream": true
        });

        // Map Tools (Gemini Format -> Ollama/OpenAI Format)
        if let Some(tools) = &request.tools {
            let mut ollama_tools = Vec::new();
            for tool in tools {
                if let Some(funcs) = &tool.function_declarations {
                    for fd in funcs {
                        ollama_tools.push(json!({
                            "type": "function",
                            "function": {
                                "name": fd.name,
                                "description": fd.description,
                                "parameters": fd.parameters
                            }
                        }));
                    }
                }
            }
            if !ollama_tools.is_empty() {
                ollama_req["tools"] = json!(ollama_tools);
            }
        }

        let url = format!("{}/api/chat", base_url);
        let res = self.client.post(&url)
            .header("Authorization", format!("Bearer {}", api_key))
            .json(&ollama_req)
            .send()
            .await?;
        
        if !res.status().is_success() {
            let _ = sender.send(OpenResponseEvent::Error { 
                error: ErrorDetails { 
                    code: "provider_error".to_string(), 
                    message: format!("Ollama returned {}", res.status())
                }
            }).await;
            return Ok(());
        }

        let response_id = Uuid::new_v4().to_string();

        let _ = sender.send(OpenResponseEvent::ResponseInProgress {
             response: ResponseResource {
                 id: response_id.clone(),
                 object: "response".to_string(),
                 created_at: Utc::now().timestamp() as u64,
                 status: "in_progress".to_string(),
                 model: request.model.clone(),
                 output: vec![],
                 completed_at: None,
                 metadata: None,
             }
        }).await;

        let mut stream = res.bytes_stream();
        
        while let Some(chunk_res) = stream.next().await {
            if let Ok(chunk) = chunk_res {
                let s = String::from_utf8_lossy(&chunk);
                for line in s.split('\n') {
                    if line.trim().is_empty() { continue; }
                    if let Ok(val) = serde_json::from_str::<serde_json::Value>(line) {
                        // 0. Thinking Content (Ollama / Gemini Style)
                        if let Some(thinking) = val["message"]["thinking"].as_str() {
                             if !thinking.is_empty() {
                                 let _ = sender.send(OpenResponseEvent::OutputThinkingDelta {
                                     delta: thinking.to_string(),
                                     response_id: response_id.clone(),
                                 }).await;
                             }
                        }
                        // 1. Text Content
                        if let Some(content) = val["message"]["content"].as_str() {
                             if !content.is_empty() {
                                 let _ = sender.send(OpenResponseEvent::OutputTextDelta {
                                     item_id: "0".to_string(),
                                     output_index: 0,
                                     content_index: 0,
                                     delta: content.to_string(),
                                     response_id: response_id.clone(),
                                 }).await;
                             }
                        }
                        // 2. Tool Calls
                        if let Some(tool_calls) = val["message"]["tool_calls"].as_array() {
                            for tc in tool_calls {
                                if let Some(func) = tc.get("function") {
                                    let name = func["name"].as_str().unwrap_or_default().to_string();
                                    // arguments can be object or string in some versions, usually object for Ollama?
                                    // Ollama sends "arguments": { ... } object. OpenResponses expects string JSON.
                                    let args_val = &func["arguments"];
                                    let args_str = if args_val.is_string() {
                                        args_val.as_str().unwrap().to_string()
                                    } else {
                                        args_val.to_string()
                                    };

                                    let _ = sender.send(OpenResponseEvent::OutputItemAdded {
                                        output_index: 0,
                                        item: crate::types::OpenResponseItem::ToolCall {
                                            id: Uuid::new_v4().to_string(),
                                            object: "tool_call".to_string(),
                                            status: "done".to_string(),
                                            name,
                                            arguments: args_str
                                        },
                                        response_id: response_id.clone(),
                                    }).await;
                                }
                            }
                        }
                    }
                }
            }
        }

        Ok(())
    }
}
