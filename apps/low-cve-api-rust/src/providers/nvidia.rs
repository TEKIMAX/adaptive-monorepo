use crate::traits::Provider;
use crate::types::*;
use async_trait::async_trait;
use reqwest::Client;
use serde_json::{json, Value};
use std::env;
use std::error::Error;
use std::time::{SystemTime, UNIX_EPOCH};
use tokio::sync::mpsc::Sender;
use uuid::Uuid;
use futures::StreamExt;

pub struct NvidiaProvider {
    pub client: Client,
}

impl NvidiaProvider {
    pub fn new() -> Self {
        Self { client: Client::new() }
    }
}

#[async_trait]
impl Provider for NvidiaProvider {
    fn id(&self) -> &str {
        "nvidia"
    }

    fn model_prefix(&self) -> &str {
        "nvidia"
    }

    async fn execute(
        &self,
        req: &CreateResponseRequest,
        _headers: &axum::http::HeaderMap,
        sender: Sender<Result<OpenResponseEvent, Box<dyn Error + Send + Sync>>>,
    ) -> Result<(), Box<dyn Error + Send + Sync>> {
        let api_key = env::var("NVIDIA_API_KEY").map_err(|_| "NVIDIA_API_KEY not found")?;
        
        // Similar normalization Logic as OpenAI (Since NVIDIA is OpenAI-compatible)
        let mut messages = Vec::new();
        for item in &req.input {
            match item {
                InputItem::Message { role, content } => {
                     let text_content = match content {
                        InputContent::String(s) => s.clone(),
                        InputContent::Parts(parts) => {
                            parts.iter().map(|p| match p {
                                InputContentPart::Text { text } => text.clone(),
                                InputContentPart::InputText { text } => text.clone(),
                                InputContentPart::InlineData { .. } => "".to_string(),
                            }).collect::<Vec<_>>().join("")
                        }
                     };
                     messages.push(json!({ "role": role, "content": text_content }));
                }
            }
        }

        let body = json!({
            "model": req.model,
            "messages": messages,
            "stream": true,
            "temperature": req.temperature,
            "max_tokens": req.max_output_tokens,
        });

        // Use NVIDIA Endpoint
        let mut stream = self.client.post("https://integrate.api.nvidia.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", api_key))
            .json(&body)
            .send()
            .await?
            .bytes_stream();

        let response_id = format!("resp_{}", Uuid::new_v4());
        let item_id = format!("msg_{}", Uuid::new_v4());
        let created_at = SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs();

        // Standard Response Setup
        let response_res = ResponseResource {
            id: response_id.clone(),
            object: "response".to_string(),
            created_at,
            status: "in_progress".to_string(),
            model: req.model.clone(),
            output: vec![],
            completed_at: None,
            metadata: None,
        };

        sender.send(Ok(OpenResponseEvent::ResponseInProgress { response: response_res.clone() })).await?;

        let item = OpenResponseItem::Message {
            id: item_id.clone(),
            role: "assistant".to_string(),
            status: "in_progress".to_string(),
            content: vec![],
        };
        sender.send(Ok(OpenResponseEvent::OutputItemAdded {
            output_index: 0,
            item: item.clone(),
            response_id: response_id.clone(),
        })).await?;

        let mut buffer = String::new();

        while let Some(chunk_res) = stream.next().await {
            let chunk = chunk_res?;
            let s = String::from_utf8_lossy(&chunk);
            
            for line in s.lines() {
                if line.starts_with("data: ") {
                    let data_str = &line[6..];
                    if data_str.trim() == "[DONE]" { continue; }

                    // NVIDIA might have slight nuances but largely follows OpenAI format
                    if let Ok(json) = serde_json::from_str::<Value>(data_str) {
                         if let Some(delta) = json["choices"][0]["delta"]["content"].as_str() {
                             if !delta.is_empty() {
                                 sender.send(Ok(OpenResponseEvent::OutputTextDelta {
                                     item_id: item_id.clone(),
                                     output_index: 0,
                                     content_index: 0,
                                     delta: delta.to_string(),
                                     response_id: response_id.clone(),
                                 })).await?;
                                 buffer.push_str(delta);
                             }
                         }
                    }
                }
            }
        }

        let item_done = OpenResponseItem::Message {
            id: item_id.clone(),
            role: "assistant".to_string(),
            status: "completed".to_string(),
            content: vec![OutputContentPart::OutputText { text: buffer }],
        };
        sender.send(Ok(OpenResponseEvent::OutputItemDone {
            output_index: 0,
            item: item_done.clone(),
            response_id: response_id.clone(),
        })).await?;

        let mut completed_res = response_res.clone();
        completed_res.status = "completed".to_string();
        completed_res.completed_at = Some(SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs());
        completed_res.output = vec![item_done];

        sender.send(Ok(OpenResponseEvent::ResponseCompleted { response: completed_res })).await?;

        Ok(())
    }
}
